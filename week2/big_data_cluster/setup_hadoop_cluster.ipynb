{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Setting Up a Hadoop Cluster on Your Local Machine\n",
    "\n",
    "**Objective**: By the end of this hands-on exercise, students will have set up a single-node Hadoop cluster on an Ubuntu machine, understand the basics of the Hadoop ecosystem (HDFS, MapReduce, YARN), and run a simple MapReduce job and an Apache Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction to Big Data and Distributed Systems\n",
    "\n",
    "**Description**: Big Data refers to large and complex datasets that traditional data processing software cannot handle efficiently. Distributed computing involves dividing these large data sets into smaller chunks and processing them on multiple machines simultaneously.\n",
    "\n",
    "Key Concepts:\n",
    "- Volume: The size of the data.\n",
    "\n",
    "- Velocity: The speed at which data is generated.\n",
    "\n",
    "- Variety: Different types of data (structured, unstructured).\n",
    "\n",
    "- Veracity: Uncertainty of data.\n",
    "\n",
    "Distributed Systems:\n",
    "- A cluster of machines (nodes) works together to process data faster.\n",
    "\n",
    "- Hadoop is one such system built for distributed processing of large data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up a Hadoop Cluster on Ubuntu\n",
    "\n",
    "**Pre-requisites**:\n",
    "- Ubuntu (Virtual Machine or Native Installation)\n",
    "\n",
    "- Java installed (`sudo apt install openjdk-8-jdk`)\n",
    "\n",
    "- SSH setup (`sudo apt install openssh-server`)\n",
    "\n",
    "### Task 1: Install Hadoop\n",
    "1. Download Hadoop: Download the latest Hadoop binaries from the official Apache Hadoop site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract Hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ tar -xvf hadoop-3.3.5.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set Environment Variables: Add the following to your `~/.bashrc` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export HADOOP_HOME=/path/to/hadoop-3.3.5\n",
    "export PATH=$PATH:$HADOOP_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Configure Hadoop:\n",
    "- Update `core-site.xml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://localhost:9000</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update `hdfs-site.xml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configure `mapred-site.xml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configure `yarn-site.xml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Format the NameNode: Run the following command to format the NameNode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Start Hadoop Services: Start the NameNode and DataNode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ start-dfs.sh\n",
    "$ start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the Hadoop services are running by visiting:\n",
    "\n",
    "- HDFS Web UI: `http://localhost:9870`\n",
    "\n",
    "- YARN Resource Manager: `http://localhost:8088`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hadoop Ecosystem: HDFS, MapReduce, YARN\n",
    "### Task 2: Introduction to HDFS (Hadoop Distributed File System)\n",
    "\n",
    "- HDFS: The distributed file system for storing large datasets across multiple nodes.\n",
    "\n",
    "1. Create a Directory in HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -mkdir /user\n",
    "$ hdfs dfs -mkdir /user/student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy Files to HDFS: Upload a sample dataset to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -put /path/to/local/file.txt /user/student/file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List Files in HDFS: Check the files in HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -ls /user/student/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: MapReduce Overview. Implement a Simple MapReduce Job\n",
    "- MapReduce: A programming model used for processing large data sets with a distributed algorithm on a cluster.\n",
    "\n",
    "1. Create a text file (word count example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ echo -e \"Hello\\nWorld\\nHello\\nHadoop\" > words.txt\n",
    "$ hdfs dfs -put words.txt /user/student/words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a Python MapReduce Job: Create a Python file named `wordcount.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordCount.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the Job: Execute the MapReduce job on the dataset in HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ python3 wordcount.py -r hadoop hdfs:///user/student/words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
