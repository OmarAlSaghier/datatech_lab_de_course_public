{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction to Apache Spark and Setting up Spark to the Cluster\n",
    "\n",
    "### Task 1: Install and Run Apache Spark\n",
    "\n",
    "1. Download Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ wget https://downloads.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "$ tar -xzf spark-3.4.0-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set Environment Variables for Spark: Add the following to your `~/.bashrc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export SPARK_HOME=/path/to/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Start and verify the Spark Shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ SPARK_HOME/bin/spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Case Study: Setting Up a Big Data Ecosystem for a Retail Business. Introduction to Spark Basics Using pyspark-shell\n",
    "\n",
    "**Scenario**:\n",
    "You are tasked with building a big data ecosystem for a fictional online retail business. The business wants to analyze large volumes of sales data to gain insights into customer behavior, popular products, and seasonal trends. You will use Hadoop and Spark to set up this big data environment, process data, and perform basic analysis.\n",
    "\n",
    "In this exercise, you'll focus on using Apache Spark to process and analyze customer sales data in real-time using the `pyspark-shell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Starting & testing pyspark-shell\n",
    "**Description**: In this task, students will use the `pyspark-shell` to interact with data and perform basic Spark operations.\n",
    "\n",
    "1. Launch `pyspark-shell`: Start the interactive PySpark shell from your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ SPARK_HOME/bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spark Context: When the shell starts, a `SparkContext` named `sc` is automatically available. This is the entry point to Spark and allows you to interact with data in a distributed manner.\n",
    "\n",
    "Check if SparkContext is running or you can overwire it by creating a new SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    ">>> sc\n",
    ">>> # OR:\n",
    ">>> sc = SparkContext(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run Simple Spark script (Word Count Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Word Count\")\n",
    "\n",
    "text_file = sc.textFile(\"hdfs:///user/student/words.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split()) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.saveAsTextFile(\"hdfs:///user/student/output/wordcount.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Loading and handeling Data into Spark\n",
    "\n",
    "**Description**: Load a sample retail sales dataset into Spark. For this case study, imagine the dataset contains customer transactions with columns like `customer_id`, `product`, `category`, and `amount_spent`.\n",
    "\n",
    "1. Create Sample Retail Data: In this example, we'll simulate a small dataset directly in the shell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"1001\", \"Laptop\", \"Electronics\", 1200.00),\n",
    "    (\"1002\", \"Smartphone\", \"Electronics\", 800.00),\n",
    "    (\"1003\", \"Shoes\", \"Fashion\", 150.00),\n",
    "    (\"1004\", \"T-shirt\", \"Fashion\", 20.00),\n",
    "    (\"1005\", \"Book\", \"Books\", 25.00)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Parallelize the Data**: Use Spark's `parallelize` function to create an RDD (Resilient Distributed Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Display the Data**: Show the first few records from the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing Basic Transformations:**\n",
    "\n",
    "**Description**: Transformations in Spark are operations that create new RDDs from existing ones. Common transformations include map, filter, and reduceByKey.\n",
    "\n",
    "4. Map Transformation: Use the map function to extract the product categories and sales amounts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_amounts = rdd.map(lambda x: (x[2], x[3]))\n",
    "categories_amounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ReduceByKey Transformation: Aggregate total sales by product category using `reduceByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_by_category = categories_amounts.reduceByKey(lambda x, y: x + y)\n",
    "total_sales_by_category.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing Actions:**\n",
    "\n",
    "**Description**: Actions trigger the execution of transformations and return results. Common actions include `collect`, `count`, and `take`.\n",
    "\n",
    "6. Count the Number of Transactions: Use the count action to find the number of transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transactions = rdd.count()\n",
    "print(f\"Total Transactions: {total_transactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Display a Sample of the Data: Use the `take` action to show a few transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = rdd.take(3)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Analyzing Retail Sales Data:\n",
    "\n",
    "**Description**: Now, students will apply the transformations and actions theyâ€™ve learned to answer specific business questions for the retail business.\n",
    "\n",
    "1. **What is the Total Revenue?** Calculate the total revenue from all sales:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_revenue = rdd.map(lambda x: x[3]).sum()\n",
    "print(f\"Total Revenue: ${total_revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **What is the Total Revenue by Category?** Calculate total sales revenue for each product category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_by_category = rdd.map(lambda x: (x[2], x[3])).reduceByKey(lambda x, y: x + y)\n",
    "total_sales_by_category.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Which Category Has the Highest Sales?** Find the product category with the highest sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_sales_category = total_sales_by_category.max(lambda x: x[1])\n",
    "print(f\"Highest Sales Category: {highest_sales_category[0]} with ${highest_sales_category[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Key Operations:\n",
    "\n",
    "- **RDD**: Resilient Distributed Dataset, the fundamental data structure in Spark.\n",
    "\n",
    "- **Transformations**: Operations that return a new RDD (e.g., `map`, `filter`, `reduceByKey`).\n",
    "\n",
    "- **Actions**: Operations that trigger the execution of transformations and return results (e.g., `collect`, `count`, `sum`).\n",
    "\n",
    "\n",
    "\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
