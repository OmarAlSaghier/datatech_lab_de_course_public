# DataTech Labs Representing a "Data Engineering" Course
This repository contains the course materials for a Data Engineering course. The course is designed to provide students with hands-on experience in data engineering, including Python programming, SQL, and data processing. The course is divided into several modules, each covering a different aspect of data engineering.

<div style="text-align: center;">
    <img src="images/datatech_labs_logo.png" alt="DataTech Labs" width="600" height="400">
</div>

## Course Description
**Course Title**: Foundations of Data Engineering and Big Data Systems

This comprehensive course introduces students to the principles and practices of data engineering and big data systems. Students will learn to design, implement, and maintain large-scale data processing systems, gaining hands-on experience with industry-standard tools and technologies. The course covers data modeling, ETL processes, distributed computing, data warehousing, and real-time data processing.

The course is divided into the following modules:
- [Week 1: Introduction to Data Engineering & Data Modeling]
- [Week 2: Data Pipelines and Big Data Processing]
- [Week 3: Data Lakes and Data Warehousing With Modern Data Architectures]
- [Week 4: Advanced Data Processing with Spark and Streaming Pipelines]
- [Week 5: Data Automation, Orchestration and Visualization]
- [Week 6: Cloud-Based Big Data Services]
- [Week 7: Project Week]


### **Learning Objectives**:

By the end of this course, students will be able to:
1. Understand the fundamentals of data engineering and its role in modern organizations
2. Design and implement efficient data pipelines and ETL processes
3. Work with distributed computing frameworks like Hadoop and Spark
4. Develop skills in data modeling and database design for big data systems
5. Implement and manage data warehouses and data lakes
6. Understand and implement real-time data processing systems
7. Utilize cloud-based big data services and technologies
8. Develop a comprehensive big data project for a real-world problem

## Weekly Breakdown:

### Week 1: Introduction to Data Engineering & Data Modeling
- Introduction to Data Engineering
    - Overview of data engineering and its importance
    - The data engineering ecosystem and toolchain
    - Roles and responsibilities of a data engineer
    - Basic data engineering concepts and terminology

    Reading: "Fundamentals of Data Engineering" by Joe Reis and Matt Housley (Chapters 1-2)

- Data Modeling and Database Design
    - Relational vs. NoSQL databases
    - Data modeling techniques for big data
    - Introduction to distributed database systems
    - Hands-on: Designing a data model for a sample use case

    Reading: "Data Model Patterns: Conventions of Thought" by David C. Hay (Chapters 1-3)


### Week 2: Data Pipelines and Big Data Processing
- ETL Processes and Data Pipelines
    - Understanding Extract, Transform, Load (ETL) processes
    - Designing efficient data pipelines
    - Data quality and data cleansing techniques
    - Hands-on: Building a simple ETL pipeline using Python

    Reading: "Building Data Pipelines with Python" by Kirrill Pomogajko (Chapters 1-4)

- Introduction to Big Data & Distributed Computing
    - Principles of distributed systems
    - The Hadoop ecosystem: HDFS, MapReduce, YARN
    - Introduction to Apache Spark
    - Hands-on: Setting up a Hadoop cluster and running basic operations

    Reading: "Hadoop: The Definitive Guide" by Tom White (Chapters 1-3)


### Week 3: Data Lakes and Data Warehousing With Modern Data Architectures
- Data Lakes Design and Implementation
    - Understanding data lakes and their benefits
    - Data lake design patterns and best practices
    - Introduction to Delta Lake and data lakehouse concept
    - Hands-on: Setting up a data lake using cloud storage

    Reading: "Data Lakes for Dummies" by Alan R. Simon (Chapters 1-5)

- Data Warehousing and Modern Data Architectures
    - Data warehouse architectures and design principles
    - Dimensional modeling and star schemas
    - OLAP concepts and cube operations
    - Hands-on: Designing and implementing a simple data warehouse

    Reading: "The Data Warehouse Toolkit" by Ralph Kimball and Margy Ross (Chapters 1-4)


### Week 4: Advanced Data Processing with Spark and Streaming Pipelines
- Advanced Spark Programming
    - Spark RDDs, DataFrames, and Datasets
    - Spark SQL and analytics
    - Machine learning with Spark MLlib
    - Hands-on: Implementing data transformations and analytics with Spark

    Reading: "Learning Spark" by Jules S. Damji et al. (Chapters 1-5)

- Streaming Data Processing
    - Introduction to stream processing concepts
    - Apache Kafka for real-time data ingestion
    - Stream processing with Spark-Streaming
    - Stream processing with Apache Flink
    - Hands-on: Implementing a real-time data pipeline with Kafka and Flink

    Reading: "Kafka: The Definitive Guide" by Neha Narkhede et al. (Chapters 1-4)


### Week 5: Data Automation, Orchestration and Visualization
- Data Automation and Orchestration
    - Workflow automation with Apache Airflow
    - Job scheduling and monitoring
    - Data pipeline orchestration best practices
    - Hands-on: Building and scheduling data workflows with Airflow

    Reading: "Data Pipelines Pocket Reference" by Christopher Groskopf (Chapters 1-3)

- Data Visualization and Reporting
    - Data Visualization Principles
    - Reporting Tools (Superset, Power BI)
    - Create effective data visualizations.
    - Hands-on: Use reporting tools for data presentation.

    Readings: Tutorials on Superset or Power BI


### Week 6: Cloud-Based Big Data Services
- Cloud-based Big Data Services
    - Overview of cloud computing for big data
    - Amazon Web Services (AWS) big data services
    - Microsoft Azure (Azure) big data services
    - Hands-on: Deploying a big data application on a cloud platform

    Reading: "Big Data Analytics on Cloud" by Pethuru Raj et al. (Chapters 1-4)


### Week 7: Project Week
- Course Project Presentations and Review
    - Student project presentations
    - Course review and future learning paths
    - Industry guest speaker session
    - Q&A and feedback session


### Required Technologies:
- Python 3.x
- Apache Hadoop
- Apache Spark
- Apache Kafka
- Apache Flink
- SQL database (e.g., PostgreSQL)
- NoSQL database (e.g., MongoDB)
- Cloud platform account (AWS or GCP)
- Data visualization tool (Superset or Power BI)
