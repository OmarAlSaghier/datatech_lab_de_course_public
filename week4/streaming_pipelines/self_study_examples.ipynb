{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study examples: Real-Time Streaming Pipeline\n",
    "\n",
    "In this step, weâ€™ll combine Kafka, Spark-Streaming, and Flink to build a full data streaming pipeline. The pipeline will consume data from an external API, produce it to a Kafka topic, process it using both Spark-Streaming and Flink, and output the results back to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Building the Pipeline\n",
    "\n",
    "1. **Step 1: Produce Data from an External API to Kafka**. Use a free online API (e.g., a random quote generator) to fetch data and produce it to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "response = requests.get('https://api.quotable.io/random')\n",
    "data = response.json()\n",
    "\n",
    "# Produce data to Kafka\n",
    "producer.send(\n",
    "    'quotes-topic',\n",
    "    data\n",
    ")\n",
    "producer.flush()\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Step 2: Consume and Process with Spark-Streaming**. Spark-Streaming job will listen to the quotes-topic Kafka topic, process the data, and produce results to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaQuoteProcessor\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "quotes_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"quotes-topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Process the data (e.g., filter quotes by author)\n",
    "processed_df = quotes_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .filter(col(\"value\").contains(\"Einstein\"))\n",
    "\n",
    "# Write back to Kafka\n",
    "query = processed_df.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"processed-quotes-topic\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Step 3: Consume and Process with Flink**. Flink will consume from the quotes-topic and produce the processed output back to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "table_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Consume from Kafka\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE quotes_source (\n",
    "        quote STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'quotes-topic',\n",
    "        'properties.bootstrap.servers' = 'localhost:9092',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Process data (e.g., count the number of quotes by a specific author)\n",
    "result = table_env.sql_query(\"\"\"\n",
    "    SELECT quote, COUNT(*) FROM quotes_source\n",
    "    WHERE quote LIKE '%Einstein%'\n",
    "    GROUP BY quote\n",
    "\"\"\")\n",
    "\n",
    "# Output to Kafka\n",
    "result.execute_insert(\"kafka_output_topic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another real-time streaming pipeline example to be studied and implemented individually:\n",
    "\n",
    "https://github.com/OmarAlSaghier/realtime_analysis_voting_project\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this hands-on exercise, students learned how to:\n",
    "\n",
    "1. Set up and work with Apache Kafka using both CLI and Python.\n",
    "\n",
    "2. Implement real-time stream processing using Spark-Streaming and PySpark.\n",
    "\n",
    "3. Use Apache Flink to process data streams and perform real-time SQL analytics.\n",
    "\n",
    "4. Build an end-to-end real-time streaming pipeline using Kafka, Spark-Streaming, and Flink, integrating external APIs, and processing data in real-time.\n",
    "\n",
    "This comprehensive pipeline demonstrates how to manage and process real-time data efficiently across multiple systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
