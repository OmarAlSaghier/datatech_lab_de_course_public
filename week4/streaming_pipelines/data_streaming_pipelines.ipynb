{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Building Data Streaming Pipelines\n",
    "\n",
    "**Objective**: Students will learn how to build real-time data streaming pipelines using Apache Kafka, Spark-Streaming (PySpark), and Apache Flink. Each tool will be introduced separately with individual hands-on tasks, followed by integrating all three tools into a single real-time streaming pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Apache Kafka\n",
    "\n",
    "**Introduction to Kafka**\n",
    "\n",
    "Apache Kafka is a distributed streaming platform used to publish, subscribe, store, and process real-time event streams. In this step, we will start by using Kafka CLI commands and then programmatically interact with Kafka using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Using Kafka CLI Commands\n",
    "\n",
    "1. Start Zookeeper: Kafka requires Zookeeper to manage brokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start Kafka Broker: Start the Kafka broker after Zookeeper is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List current topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ kafka-topics.sh --bootstrap-server localhost:9092 --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a Kafka Topic: Use the Kafka CLI to create a topic for your streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Produce Messages to Kafka: Send messages to the Kafka topic from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Consume Messages from Kafka: Read messages from the Kafka topic in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ kafka-console-consumer.sh --topic test-topic --bootstrap-server localhost:9092 --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Kafka Programmatically with Python\n",
    "\n",
    "1. Install Kafka Python Library: Install the `kafka-python` library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Producer Script: Write a Python script to produce messages to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    producer.send('test-topic', {'number': i})\n",
    "    producer.flush()\n",
    "\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consumer Script: Write a Python script to consume messages from a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test-topic',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    print(message.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Spark-Streaming Using PySpark\n",
    "\n",
    "**Introduction to Spark-Streaming**\n",
    "\n",
    "Spark-Streaming provides real-time stream processing capabilities built on top of Apache Spark. In this task, we'll create a streaming job using PySpark to process CSV files arriving in a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Processing Data Using Spark-Streaming\n",
    "\n",
    "1. **Initialize Spark Session**: Create a Spark session for your streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesDataStreamingJob\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Monitor an HDFS Directory**: Set up the streaming context to listen to an HDFS directory and process new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the streaming data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the streaming data from the HDFS directory\n",
    "csv_stream = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"hdfs:///user/datatech-labs/streaming-data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Transformation**:\n",
    "Perform the required transformations on the streaming data. For instance, calculate total sales (total_amount) per customer_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer_id and calculate total sales\n",
    "processed_data = csv_stream.groupBy(\"customer_id\").sum(\"total_amount\") \\\n",
    "    .withColumnRenamed(\"sum(total_amount)\", \"total_sales\") \\\n",
    "    .orderBy(\"total_sales\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Write the Stream Output**: Write the output to the console in real-time for debugging purposes. You can later modify this to write to a database or another HDFS location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = processed_data.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Homework:\n",
    "5. Write the above code snippets to one script, and submit it to the Spark cluster using `sprak-submit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apache Flink\n",
    "\n",
    "**Introduction to Apache Flink**\n",
    "\n",
    "Apache Flink is a powerful stream-processing framework that enables real-time data analytics. In this task, weâ€™ll set up a Flink pipeline to consume data from Kafka, process it using Flink SQL, and create a virtual table for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Consume from Kafka and Process with Flink\n",
    "\n",
    "1. Install Flink cluster with the script `./install-flink-script.sh` and open the UI at `localhost:8081`\n",
    "\n",
    "2. Optionally Install Flink Python API: Install the PyFlink package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ pip install apache-flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run (submit) the following example job that comes pre-installed with Apache Flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ echo -e \"hello world\\napache flink\\nflink cluster\\nhello flink\" > ~/Desktop/sample.txt\n",
    "\n",
    "$ ./bin/flink run examples/batch/WordCount.jar \\\n",
    "    --input ~/Desktop/sample.txt \\\n",
    "    --output ~/Desktop/sample-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Set up Kafka Consumer in Flink**:\n",
    "\n",
    "Create a Flink job to consume messages from Kafka, and submit it to the installed Flink Clsuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "table_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Create Kafka Source Table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE kafka_source (\n",
    "        number INT\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'flink-topic',\n",
    "        'properties.bootstrap.servers' = 'localhost:9092',\n",
    "        'properties.group.id' = 'flink-consumer-group',\n",
    "        'scan.startup.mode' = 'earliest-offset',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "result = table_env.sql_query(\"\"\"\n",
    "    SELECT number, COUNT(*) \n",
    "    FROM kafka_source \n",
    "    GROUP BY number\n",
    "\"\"\")\n",
    "\n",
    "# Print the results\n",
    "result.execute().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make the file executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ chmod +x flink_scripts/flink_kafka_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Submit the script using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ flink run -py flink_scripts/flink_kafka_example.py\n",
    "\n",
    "# or with full command:\n",
    "$ /opt/flink/bin/flink run -py flink_scripts/flink_kafka_example.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "### **Conclusion**\n",
    "\n",
    "In this hands-on exercise, students learned how to:\n",
    "\n",
    "1. Set up and work with Apache Kafka using both CLI and Python.\n",
    "\n",
    "2. Implement real-time stream processing using Spark-Streaming and PySpark.\n",
    "\n",
    "3. Use Apache Flink to process data streams and perform real-time SQL analytics.\n",
    "\n",
    "4. Build an end-to-end real-time streaming pipeline using Kafka, Spark-Streaming, and Flink, integrating external APIs, and processing data in real-time.\n",
    "\n",
    "This comprehensive pipeline demonstrates how to manage and process real-time data efficiently across multiple systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
