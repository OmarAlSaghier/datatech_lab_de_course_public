{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Building Data Streaming Pipelines\n",
    "\n",
    "**Objective**: Students will learn how to build real-time data streaming pipelines using Apache Kafka, Spark-Streaming (PySpark), and Apache Flink. Each tool will be introduced separately with individual hands-on tasks, followed by integrating all three tools into a single real-time streaming pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Apache Kafka\n",
    "\n",
    "**Introduction to Kafka**\n",
    "\n",
    "Apache Kafka is a distributed streaming platform used to publish, subscribe, store, and process real-time event streams. In this step, we will start by using Kafka CLI commands and then programmatically interact with Kafka using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Using Kafka CLI Commands\n",
    "\n",
    "1. Start Zookeeper: Kafka requires Zookeeper to manage brokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start Kafka Broker: Start the Kafka broker after Zookeeper is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a Kafka Topic: Use the Kafka CLI to create a topic for your streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Produce Messages to Kafka: Send messages to the Kafka topic from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Consume Messages from Kafka: Read messages from the Kafka topic in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ bin/kafka-console-consumer.sh --topic test-topic --bootstrap-server localhost:9092 --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Kafka Programmatically with Python\n",
    "\n",
    "1. Install Kafka Python Library: Install the `kafka-python` library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Producer Script: Write a Python script to produce messages to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "for i in range(100):\n",
    "    producer.send('test-topic', {'number': i})\n",
    "    producer.flush()\n",
    "\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consumer Script: Write a Python script to consume messages from a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('test-topic',\n",
    "                         bootstrap_servers='localhost:9092',\n",
    "                         value_deserializer=lambda m: json.loads(m.decode('utf-8')))\n",
    "\n",
    "for message in consumer:\n",
    "    print(message.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Spark-Streaming Using PySpark\n",
    "\n",
    "**Introduction to Spark-Streaming**\n",
    "\n",
    "Spark-Streaming provides real-time stream processing capabilities built on top of Apache Spark. In this task, we'll create a streaming job using PySpark to process CSV files arriving in a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Processing Data Using Spark-Streaming\n",
    "\n",
    "1. Initialize Spark Session: Create a Spark session for your streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"StreamingJob\"). \\\n",
    "    getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Monitor an HDFS Directory: Set up the streaming context to listen to an HDFS directory and process new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_stream = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(\"customer_id INT, sales DOUBLE\") \\\n",
    "    .load(\"hdfs://namenode:9000/path/to/streaming/directory\")\n",
    "\n",
    "# Transformation\n",
    "processed_data = csv_stream.groupBy(\"customer_id\").sum(\"sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write the Stream Output: Write the output to the console in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = processed_data.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write the above code snippets to one script, and submit it to the Spark cluster using `sprak-submit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apache Flink\n",
    "\n",
    "**Introduction to Apache Flink**\n",
    "\n",
    "Apache Flink is a powerful stream-processing framework that enables real-time data analytics. In this task, we’ll set up a Flink pipeline to consume data from Kafka, process it using Flink SQL, and create a virtual table for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Consume from Kafka and Process with Flink\n",
    "\n",
    "1. Install Flink Python API: Install the PyFlink package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ pip install apache-flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up Kafka Consumer in Flink: Create a Flink job to consume messages from Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "table_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Create Kafka Source Table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE kafka_source (\n",
    "        number INT\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'test-topic',\n",
    "        'properties.bootstrap.servers' = 'localhost:9092',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a Flink SQL Query: Use Flink SQL to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = table_env.sql_query(\"\"\"\n",
    "    SELECT number, COUNT(*) FROM kafka_source GROUP BY number\n",
    "\"\"\")\n",
    "\n",
    "# Print the results\n",
    "result.execute().print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Real-Time Streaming Pipeline\n",
    "\n",
    "In this step, we’ll combine Kafka, Spark-Streaming, and Flink to build a full data streaming pipeline. The pipeline will consume data from an external API, produce it to a Kafka topic, process it using both Spark-Streaming and Flink, and output the results back to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Building the Pipeline\n",
    "\n",
    "1. **Step 1: Produce Data from an External API to Kafka**. Use a free online API (e.g., a random quote generator) to fetch data and produce it to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "response = requests.get('https://api.quotable.io/random')\n",
    "data = response.json()\n",
    "\n",
    "# Produce data to Kafka\n",
    "producer.send(\n",
    "    'quotes-topic',\n",
    "    data\n",
    ")\n",
    "producer.flush()\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Step 2: Consume and Process with Spark-Streaming**. Spark-Streaming job will listen to the quotes-topic Kafka topic, process the data, and produce results to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaQuoteProcessor\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "quotes_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"quotes-topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Process the data (e.g., filter quotes by author)\n",
    "processed_df = quotes_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .filter(col(\"value\").contains(\"Einstein\"))\n",
    "\n",
    "# Write back to Kafka\n",
    "query = processed_df.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"processed-quotes-topic\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Step 3: Consume and Process with Flink**. Flink will consume from the quotes-topic and produce the processed output back to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "table_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Consume from Kafka\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE quotes_source (\n",
    "        quote STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'quotes-topic',\n",
    "        'properties.bootstrap.servers' = 'localhost:9092',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Process data (e.g., count the number of quotes by a specific author)\n",
    "result = table_env.sql_query(\"\"\"\n",
    "    SELECT quote, COUNT(*) FROM quotes_source\n",
    "    WHERE quote LIKE '%Einstein%'\n",
    "    GROUP BY quote\n",
    "\"\"\")\n",
    "\n",
    "# Output to Kafka\n",
    "result.execute_insert(\"kafka_output_topic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another real-time streaming pipeline example to be studied and implemented individually:\n",
    "\n",
    "https://github.com/OmarAlSaghier/realtime_analysis_voting_project\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this hands-on exercise, students learned how to:\n",
    "\n",
    "1. Set up and work with Apache Kafka using both CLI and Python.\n",
    "\n",
    "2. Implement real-time stream processing using Spark-Streaming and PySpark.\n",
    "\n",
    "3. Use Apache Flink to process data streams and perform real-time SQL analytics.\n",
    "\n",
    "4. Build an end-to-end real-time streaming pipeline using Kafka, Spark-Streaming, and Flink, integrating external APIs, and processing data in real-time.\n",
    "\n",
    "This comprehensive pipeline demonstrates how to manage and process real-time data efficiently across multiple systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
