{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Advanced Spark Programming Using PySpark\n",
    "\n",
    "Objective: This exercise introduces students to advanced Spark programming with PySpark. It covers RDDs, DataFrames, Datasets, Spark SQL, data analytics, and machine learning with Spark MLlib. By the end, students will have hands-on experience implementing transformations and performing analytics using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Spark RDDs, DataFrames, and Datasets\n",
    "\n",
    "**What are RDDs, DataFrames, and Datasets?**\n",
    "\n",
    "- RDDs (Resilient Distributed Datasets): A fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "- DataFrames: Distributed collection of data organized into named columns.\n",
    "\n",
    "- Datasets: An extension of DataFrames providing type-safety and object-oriented programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create RDDs and Perform Basic Operations\n",
    "\n",
    "1. Initialize a Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# simple spark session run locally\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"BasicSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark session run on yarn with some configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"AdvancedSpark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '3') \\\n",
    "    .config('spark.cores.max', '3') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create an RDD**: Create an RDD from a list of numbers and perform basic transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Map transformation: Multiply each element by 2\n",
    "rdd_mapped = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Collect results\n",
    "print(rdd_mapped.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Create a DataFrame**: Convert the RDD into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd_mapped.toDF([\"value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Create a Dataset (Spark 2.x+)**: Datasets in PySpark are part of DataFrames, so DataFrame operations are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrames as Datasets\n",
    "df.createOrReplaceTempView(\"numbers\")\n",
    "dataset = spark.sql(\"SELECT value FROM numbers\")\n",
    "dataset.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Spark DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Perform DataFrame Transformations and Actions\n",
    "\n",
    "1. Join DataFrames: Assume you have two DataFrames: `sales_df` (sales data) and `product_df` (product information). Join them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.csv(\n",
    "    \"hdfs:///path/to/sales_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "product_df = spark.read.csv(\n",
    "    \"hdfs:///path/to/product_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "retail_df = sales_df.join(product_df, sales_df[\"product_id\"] == product_df[\"product_id\"])\n",
    "retail_df.show(5)\n",
    "\n",
    "# write to hdfs\n",
    "retail_df.write.csv(\n",
    "    \"hdfs:///path/to/output.csv\",\n",
    "    header=True,\n",
    "    mode=\"overwrite\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DataFrame Transformations: Use transformation functions like `withColumn`, `drop`, `distinct`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with a calculated value (total cost)\n",
    "retail_df = retail_df.withColumn(\"total_cost\", retail_df[\"quantity\"] * retail_df[\"price\"])\n",
    "retail_df.show(5)\n",
    "\n",
    "# Drop columns\n",
    "retail_df = retail_df.drop(\"discount\")\n",
    "retail_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cache and Persist: Cache a DataFrame to memory for repeated access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.cache()\n",
    "retail_df.count()  # Action to trigger caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Explore DataFrames in Detail\n",
    "\n",
    "1. Load a DataFrame from a CSV File: Load retail sales data from a CSV file into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df = spark.read.csv(\n",
    "    \"hdfs:///path/to/retail_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "retail_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore DataFrame Schema: Check the schema of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select and Filter Data: Perform basic operations on DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "retail_df.select(\"product_id\", \"quantity\", \"price\").show()\n",
    "\n",
    "# Filter rows where quantity is greater than 10\n",
    "retail_df.filter(retail_df[\"quantity\"] > 10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GroupBy and Aggregations: Perform aggregations on the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.groupBy(\"product_id\") \\\n",
    "    .agg({\n",
    "        \"quantity\": \"sum\",\n",
    "        \"price\": \"avg\"\n",
    "    }) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Spark SQL and Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Write SQL Queries on DataFrames\n",
    "\n",
    "1. Create Temporary Views for SQL Queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.createOrReplaceTempView(\"retail_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Execute SQL Queries: Run SQL queries on the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT product_id, SUM(quantity) as total_quantity \n",
    "    FROM retail_data \n",
    "    GROUP BY product_id\n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Join Using SQL: Write SQL for joining the DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "product_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT s.product_id, p.product_name, SUM(s.quantity) as total_quantity\n",
    "    FROM sales s\n",
    "    JOIN products p ON s.product_id = p.product_id\n",
    "    GROUP BY s.product_id, p.product_name\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Connecting to Hive and Reading Data from Hive Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Query Data from Hive Tables\n",
    "\n",
    "1. **Connect to Hive**: Ensure that Hive is properly configured and that the Hive Metastore is accessible from Spark.\n",
    "\n",
    "**Refer to \"Hive_installation\" in week3 for more details.**\n",
    "\n",
    "2. Read Hive Table into Spark: Query data from Hive and load it into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Hive Exercise\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from a Hive table\n",
    "hive_df = spark.sql(\"SELECT * FROM retail_dw.sales\")\n",
    "\n",
    "hive_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write Data to Hive: Write data back to a Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich data\n",
    "enriched_hive_df = hive_df.withColumn(\"total_cost\", hive_df[\"quantity\"] * hive_df[\"price\"])\n",
    "\n",
    "enriched_hive_df.write. \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    saveAsTable(\"retail_dw.enriched_sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Machine Learning with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Build a Simple Machine Learning Model Using Spark MLlib\n",
    "\n",
    "1. Load a Dataset: Load a dataset for machine learning (e.g., customer data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load customer dataset\n",
    "customer_df = spark.read.csv(\"/path/to/customer_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select features and label\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"Income\"], outputCol=\"features\")\n",
    "final_df = assembler.transform(customer_df).select(\"features\", \"SpendingScore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a Linear Regression Model: Use MLlib to train a regression model to predict customer spending based on age and income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"SpendingScore\")\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(final_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(final_df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate the Model: Evaluate the performance of the model using metrics such as RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"SpendingScore\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implementing Data Transformations and Analytics with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Perform Data Transformations and Analytics\n",
    "\n",
    "1. Write a Transformation Pipeline: Chain multiple transformations to create a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Transformations: StringIndexer and OneHotEncoder for categorical variables\n",
    "indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"product_index\", outputCol=\"product_encoded\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "pipeline_model = pipeline.fit(retail_df)\n",
    "transformed_data = pipeline_model.transform(retail_df)\n",
    "transformed_data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analytics and Aggregation: Perform advanced analytics using `groupBy`, `windowing`, and aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# Window function: running total of sales per product\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "retail_df \\\n",
    "    .withColumn(\"running_total\", sum(\"total_cost\") \\\n",
    "    .over(window_spec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Submitting PySpark Jobs with spark-submit\n",
    "\n",
    "In this step, students will learn how to use the `spark-submit` command to run PySpark applications on a cluster, how to include dependencies, and how to monitor the job's progress through the Spark UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Submit a PySpark Script Using `spark-submit`\n",
    "\n",
    "1. Create a PySpark Script: Write a PySpark script and save it as `retail_analysis.py`. Here’s a basic script to process retail data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Retail Data Analysis\"). \\\n",
    "    getOrCreate()\n",
    "\n",
    "# Load retail sales data from CSV\n",
    "retail_df = spark.read.csv(\n",
    "    \"/path/to/retail_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Perform basic transformation\n",
    "df_filtered = retail_df.filter(retail_df['Sales'] > 100)\n",
    "\n",
    "# Show results\n",
    "df_filtered.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Submit the Script Using spark-submit: Use the `spark-submit` command to submit the PySpark script to the Spark cluster.\n",
    "\n",
    "- `--master`: Specifies the master URL (in this case, `yarn` for a Hadoop cluster).\n",
    "\n",
    "- `--deploy-mode`: Defines where the driver program will run (`cluster` or `client`).\n",
    "\n",
    "- `/path/to/retail_analysis.py`: The path to the PySpark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ spark-submit \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode cluster \\\n",
    "    /path/to/retail_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Including Dependencies in spark-submit\n",
    "\n",
    "Sometimes, you need to include external dependencies (such as additional libraries) when submitting a job. There are two common ways to do this:\n",
    "\n",
    "1. Include a Python Package: Use the `--py-files` option to add additional Python files or ZIP files that contain dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ spark-submit \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode cluster \\\n",
    "    --py-files /path/to/dependencies.zip \\\n",
    "    /path/to/retail_analysis.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Include a JAR File: If the job requires external Java libraries, use the `--jars` option to include the JAR file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ spark-submit \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode cluster \\\n",
    "    --jars /path/to/external-library.jar \\\n",
    "    /path/to/retail_analysis.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Monitor the Job Through Spark UI\n",
    "\n",
    "Spark provides a web-based UI for monitoring job execution and resource usage.\n",
    "\n",
    "1. Access the Spark UI: Once the job is submitted, the Spark UI can be accessed using the Spark master’s web interface. The default port is `4040` for client mode or `8088` for YARN Resource Manager.\n",
    "\n",
    "- If running in client mode, open a web browser and navigate to:\n",
    "    `http://<driver-host>:4040`\n",
    "\n",
    "- If running in cluster mode, access the YARN Resource Manager web interface:\n",
    "    `http://<resource-manager-host>:8088`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Monitor Job Progress:\n",
    "\n",
    "- *Stages*: View how the job is split into different stages and how tasks are executed.\n",
    "\n",
    "- *Tasks*: Monitor the completion rate of tasks and check for any failed tasks.\n",
    "\n",
    "- *Executors*: Check how much memory and CPU each executor is using.\n",
    "\n",
    "- *Storage*: See cached data, RDDs, and DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check Logs: In the Spark UI, under each job, you can access detailed logs to diagnose any failures or performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
