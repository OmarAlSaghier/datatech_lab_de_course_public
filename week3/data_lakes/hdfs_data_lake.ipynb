{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Implementing a Data Lake Using Apache HDFS\n",
    "\n",
    "**Objective**: By the end of this exercise, students will have a clear understanding of data lakes, their benefits, design patterns, and best practices. They will also set up a basic data lake using Apache HDFS on the previously created Hadoop cluster, and gain an introduction to the Delta Lake concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Data Lakes and Their Benefits\n",
    "\n",
    "**What is a Data Lake?**\n",
    "A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to structure it first, and run different types of analyticsâ€”from dashboards and visualizations to big data processing, real-time analytics, and machine learning.\n",
    "\n",
    "**Benefits of a Data Lake:**\n",
    "- Scalability: Handle massive volumes of structured, semi-structured, and unstructured data.\n",
    "- Cost-Effective Storage: Store raw data in its native format without schema enforcement.\n",
    "- Data Variety: Allows for ingestion of a wide variety of data formats.\n",
    "- Real-Time Analytics: Supports real-time or near-real-time analysis with tools like Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Lake Design Patterns and Best Practices\n",
    "\n",
    "**Design Patterns:**\n",
    "- Raw Layer (Landing Zone): Where data enters the lake. It is stored in its raw format (JSON, CSV, Parquet, etc.) without processing or transformations.\n",
    "- Cleansed Layer (Curated Zone): Cleaned and transformed data ready for querying or analysis. Often stored in more structured formats.\n",
    "- Enriched Layer (Consumption Zone): Data optimized for specific use cases, such as machine learning models or reporting.\n",
    "\n",
    "\n",
    "**Best Practices:**\n",
    "- Partitioning: Divide large datasets into partitions (e.g., by date, region) for faster access.\n",
    "- Metadata Management: Catalog data using tools like Apache Hive or AWS Glue for easier access.\n",
    "- Governance: Implement strict access controls, data lineage tracking, and logging to manage data effectively.\n",
    "- Schema-on-Read: Apply structure only when reading data, giving flexibility to store unstructured data without schema constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Introduction to Delta Lake and Data Lakehouse Concept\n",
    "\n",
    "**Delta Lake:**\n",
    "Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions to data lakes. It ensures data reliability during streaming and batch operations by providing capabilities like time travel, upserts, and deletes.\n",
    "\n",
    "**Data Lakehouse:**\n",
    "A data lakehouse combines the best features of data lakes (ability to store structured and unstructured data) and data warehouses (ability to run SQL queries efficiently) into a single architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hands-On Exercise: Setting up a Data Lake Using Apache HDFS\n",
    "\n",
    "**Pre-requisites:**\n",
    "- A Hadoop cluster running on your local machine (from the previous setup).\n",
    "- HDFS up and running.\n",
    "\n",
    "**Steps:**\n",
    "- Open the Hadoop cluster web interface (http://localhost:9870) and navigate to the \"Utilities\" tab.\n",
    "\n",
    "- Checking hdfs health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs fsck /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hdfs listing help commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hdfs listing files with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -ls -t -r /user/datatech-labs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -mkdir /user/datatech-labs/first_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove empty directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -rmdir /user/datatech-labs/first_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy files from local to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ echo \"hello from file 1\" > ~/Desktop/file_1.txt\n",
    "\n",
    "$ hdfs dfs -put Desktop/file* /user/datatech-labs/\n",
    "# or\n",
    "$ hdfs dfs -copyFromLocal Desktop/file* /user/datatech-labs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy from hdfs to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -get /user/datatech-labs/file* Desktop/test_data/\n",
    "# or\n",
    "$ hdfs dfs -copyToLocal /user/datatech-labs/file* Desktop/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy files inside hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -mkdir /user/datatech-labs/test_data/\n",
    "$ hdfs dfs -cp /user/datatech-labs/file* /user/datatech-labs/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete directory and its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -rm -r /user/datatech-labs/test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Move inside hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -mkdir /user/datatech-labs/test_data/\n",
    "$ hdfs dfs -mv /user/datatech-labs/file* /user/datatech-labs/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View file content on hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -cat /user/datatech-labs/file_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Information about a specific file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs fsck /user/datatech-labs/file_1.txt -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hdfs storage usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -df -h  #(-h for human readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hdfs storage information for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -du -h /user/datatech-labs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hdfs storage information for a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -du -s -h /user/datatech-labs/ #(is for sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
