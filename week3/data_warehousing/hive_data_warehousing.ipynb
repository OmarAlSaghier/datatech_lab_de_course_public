{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Implementing a Data Warehouse in Apache Hive Using Retail Big Data\n",
    "\n",
    "This exercise will guide students through setting up a simple Hive data warehouse on the previously created Hadoop cluster. The use case will focus on retail big data, and students will implement a dimensional model (star schema) in Hive, explore OLAP concepts, and practice Hive partitioning, bucketing, and external table creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Designing and Implementing a Hive Simple Data Warehouse\n",
    "\n",
    "**What is a Data Warehouse?**\n",
    "A data warehouse is a system used for reporting and data analysis, integrating data from multiple sources to provide a consolidated view for business intelligence.\n",
    "\n",
    "**Retail Use Case:**\n",
    "In this exercise, we will work with a sample retail dataset that contains the following tables:\n",
    "\n",
    "- Customers: Information about customers.\n",
    "- Products: Product catalog.\n",
    "- Sales: Sales transaction records.\n",
    "- Stores: Details about store locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create a Hive Database for the Data Warehouse\n",
    "\n",
    "1. Launch the Hive shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a Database: To organize the data, create a database named `retail_dw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE retail_dw;\n",
    "USE retail_dw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Hive Data Warehouse Architectures and Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create Hive Tables\n",
    "Design and implement the basic structure for your Hive data warehouse using tables for the retail use case. We'll follow the star schema design pattern for efficient querying and reporting.\n",
    "\n",
    "Create the Fact and Dimension Tables:\n",
    "\n",
    "1. Customers Dimension Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE customers (\n",
    "  customer_id INT,\n",
    "  customer_name STRING,\n",
    "  customer_email STRING,\n",
    "  customer_phone STRING,\n",
    "  customer_address STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Products Dimension Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE products (\n",
    "  product_id INT,\n",
    "  product_name STRING,\n",
    "  category STRING,\n",
    "  price DOUBLE\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "3. Stores Dimension Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE stores (\n",
    "  store_id INT,\n",
    "  store_name STRING,\n",
    "  store_city STRING,\n",
    "  store_state STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sales Fact Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE sales (\n",
    "  transaction_id INT,\n",
    "  transaction_date STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  store_id INT,\n",
    "  quantity INT,\n",
    "  total_amount DOUBLE\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Load data into tables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/customers_data.csv' INTO TABLE customers;\n",
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/products_data.csv' INTO TABLE products;\n",
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/stores_data.csv' INTO TABLE stores;\n",
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/sales_data.csv' INTO TABLE sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hive Dimensional Modeling and Star Schema\n",
    "\n",
    "In dimensional modeling, we use fact and dimension tables. The fact table (sales) stores quantitative data for analysis, while the dimension tables (customers, products, stores) provide context to the facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the Star Schema\n",
    "\n",
    "In this star schema:\n",
    "- Fact Table: `sales`\n",
    "\n",
    "- Dimension Tables: `customers`, `products`, `stores`\n",
    "\n",
    "After creating the tables in step 2, your Hive data warehouse follows a star schema with `sales` at the center of the schema, surrounded by the dimensions (customers, products, stores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hive OLAP Concepts and Cube Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Run OLAP Queries Using GROUP BY and CUBE\n",
    "\n",
    "1. Simple OLAP Query: Query to calculate total sales by product category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT p.category, SUM(s.total_amount)\n",
    "FROM sales s\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "GROUP BY p.category\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cube Operation: Use CUBE to get aggregated results across multiple dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT p.category, st.store_state, SUM(s.total_amount)\n",
    "FROM sales s\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "JOIN stores st ON s.store_id = st.store_id\n",
    "GROUP BY CUBE(p.category, st.store_state)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hive Partitioning and Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioning:**\n",
    "Partitioning in Hive allows us to split tables into smaller pieces based on certain columns, improving query performance.\n",
    "\n",
    "### Task 5: Implement Partitioning on the `Sales` Table\n",
    "\n",
    "1. Create a Partitioned Sales Table: Partition the sales data by `store_state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE sales_partitioned (\n",
    "  transaction_id INT,\n",
    "  transaction_date STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  store_id INT,\n",
    "  quantity INT,\n",
    "  total_amount DOUBLE\n",
    ")\n",
    "PARTITIONED BY (store_state STRING)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add Data to Partitions: Load data into partitions based on the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/sales_data.csv'\n",
    "INTO TABLE sales_partitioned\n",
    "PARTITION (store_state = 'CA');\n",
    "\n",
    "SELECT * FROM sales_partitioned;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketing:**\n",
    "Bucketing further divides each partition into \"buckets\" for better parallelism.\n",
    "\n",
    "### Task 6: Implement Bucketing on the Customers Table\n",
    "\n",
    "1. Create a Bucketed Customers Table: Bucket the customers table by customer_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE customers_bucketed (\n",
    "  customer_id INT,\n",
    "  customer_name STRING,\n",
    "  customer_email STRING,\n",
    "  customer_phone STRING,\n",
    "  customer_address STRING\n",
    ")\n",
    "CLUSTERED BY (customer_id) INTO 4 BUCKETS\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Data into the Bucketed Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/customers_data.csv'\n",
    "INTO TABLE customers_bucketed;\n",
    "\n",
    "SELECT * FROM customers_bucketed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: How Hive Data is Stored in HDFS\n",
    "\n",
    "Hive tables are stored as directories in HDFS, with each table represented as a directory and each partition (if partitioned) represented as subdirectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Explore HDFS Storage for Hive Tables\n",
    "\n",
    "1. Check the HDFS location of a table: You can find the storage location of Hive tables by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE FORMATTED sales_partitioned;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore the HDFS Directory: Use HDFS commands to explore the directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -ls /user/hive/warehouse/retail_dw.db/sales_partitioned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Hive Metastore Query Sample\n",
    "\n",
    "The Hive metastore stores metadata about tables. You can query the metastore using the `SHOW` and `DESCRIBE` commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Query the Hive Metastore\n",
    "\n",
    "1. View All Tables in the Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SHOW TABLES IN retail_dw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe a Tableâ€™s Schema: Get detailed information about the `sales` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE FORMATTED sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load Data from Local Path or HDFS Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Load Data into Hive Tables\n",
    "\n",
    "1. Loading Data from a Local Path: Load local data into the `products` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "LOAD DATA LOCAL INPATH './Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/products_data.csv'\n",
    "INTO TABLE products;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loading Data from HDFS: Upload the data file to HDFS first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ hdfs dfs -put ~/Documents/datatech_labs/datatech_lab_de_course_public/week3/data_warehousing/hands_on_data/* /user/datatech-labs/hive-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then, load the HDFS data into the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "LOAD DATA INPATH '/user/datatech-labs/hive-data/products_data.csv'\n",
    "INTO TABLE products;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Hive External Tables\n",
    "\n",
    "External tables in Hive allow you to manage data outside the Hive warehouse directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Create an External Table\n",
    "\n",
    "1. Create an External Table for Stores Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE external_stores (\n",
    "  store_id INT,\n",
    "  store_name STRING,\n",
    "  store_city STRING,\n",
    "  store_state STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/datatech-labs/hive-data/stores'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read Data through the External Table: Ensure the data exists at the specified HDFS location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM external_stores;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Specify Storing Format in Table Creation\n",
    "\n",
    "Hive supports various file formats like Text, ORC, and Parquet. Using efficient formats like ORC or Parquet can improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Create Tables with Different Storage Formats\n",
    "\n",
    "1. Create a Table Using `Parquet` Format: In this task, we will create a Hive table for the `sales` data and specify that it should be stored in Parquet format, which is columnar and optimized for big data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE sales_parquet (\n",
    "  transaction_id INT,\n",
    "  transaction_date STRING,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  store_id INT,\n",
    "  quantity INT,\n",
    "  total_amount DOUBLE\n",
    ")\n",
    "STORED AS PARQUET\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Load Data into the Parquet Table**: You can load data into the Parquet table just like any other Hive table. If you already have data in the `sales` table, you can insert it into the `sales_parquet` table using a simple INSERT statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO TABLE sales_parquet\n",
    "SELECT * FROM sales\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Verify the Data in HDFS: You can check the storage of the Parquet table in HDFS using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/retail_dw.db/sales_parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
