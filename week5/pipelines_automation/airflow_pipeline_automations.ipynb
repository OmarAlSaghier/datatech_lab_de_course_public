{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise: Apache Airflow for Data Pipelines Automation and Orchestration\n",
    "\n",
    "**Objective**: Students will learn how to use Apache Airflow to automate and orchestrate data pipelines. The hands-on exercise will cover workflow automation, job scheduling, monitoring, and integrating Airflow with the previously created Hadoop cluster components like HDFS, Hive, and Spark.\n",
    "\n",
    "\n",
    "**Introduction to Apache Airflow**\n",
    "\n",
    "Apache Airflow is an open-source platform for authoring, scheduling, and monitoring workflows as Directed Acyclic Graphs (DAGs). It allows for the automation of ETL (Extract, Transform, Load) jobs and other complex workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Job Scheduling and Monitoring\n",
    "\n",
    "Airflow allows users to schedule tasks using DAGs, which are Python scripts defining the sequence of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Creating a Simple Airflow DAG\n",
    "\n",
    "1. Create Your First DAG: Create a DAG file `simple_dag.py` in the Airflow DAGs directory (`~/airflow/dags/`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define DAG arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Instantiate a DAG\n",
    "dag = DAG(\n",
    "    'simple_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily'\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='current_working_dir',\n",
    "    bash_command='pwd',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "t1 >> t2 >> t3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Monitor the DAG in Airflow UI: Once the DAG is created, navigate to the Airflow UI, where you will find the DAG listed under \"DAGs\". Trigger it manually and monitor its progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building and Scheduling Data Workflows with Airflow\n",
    "\n",
    "Now that you understand how to create simple DAGs, let's build a more complex workflow, integrating tasks such as data extraction, transformation, and loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Building a Data Workflow\n",
    "\n",
    "1. Create a Data Pipeline DAG: In this example, you will create a DAG that simulates a data workflow: fetching data from an API, processing it, and loading it to HDFS. Copy the below into a file `hdfs_upload_dag.py`\n",
    "\n",
    "but before, make sure `jq` command is installed with: `sudo apt  install jq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Define default arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 11, 22),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'hdfs_data_workflow_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily'\n",
    ")\n",
    "\n",
    "# Define Python function to fetch data from API\n",
    "def fetch_data():\n",
    "    response = requests.get(\"https://dummyjson.com/users\")\n",
    "    with open('/tmp/data.json', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "# Define tasks\n",
    "fetch_task = PythonOperator(\n",
    "    task_id='fetch_data_from_api',\n",
    "    python_callable=fetch_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "process_task = BashOperator(\n",
    "    task_id='process_data',\n",
    "    bash_command='cat /tmp/data.json | jq . > /tmp/processed_data.json',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "create_dir_task = BashOperator(\n",
    "    task_id='create_hdfs_dir',\n",
    "    bash_command='hdfs dfs -mkdir -p /user/datatech-labs/airflow_processed_data',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = BashOperator(\n",
    "    task_id='load_to_hdfs',\n",
    "    bash_command='hdfs dfs -put /tmp/processed_data.json /user/datatech-labs/airflow_processed_data',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Set task dependencies\n",
    "fetch_task >> process_task >> create_dir_task >> load_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Run the DAG**: Monitor the tasks through the Airflow UI as the data flows through fetching, processing, and loading stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Check Logs**: In the Airflow UI, go to the \"Graph View\" or \"Tree View\" of a DAG and click on a task to access the logs. Logs contain detailed information about task execution, including errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Airflow Operators\n",
    "\n",
    "Operators are the building blocks of DAGs. Airflow has multiple types of operators for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using Airflow Operators\n",
    "\n",
    "1. **BashOperator**: This operator allows you to run bash commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "bash_task = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **PythonOperator**: Use this operator to execute Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def my_function():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    task_id='run_python_function',\n",
    "    python_callable=my_function,\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Airflow Sensors\n",
    "\n",
    "Sensors are special types of operators that wait for a certain condition to be met before continuing execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Folder Listening with Airflow Sensor\n",
    "\n",
    "1. HDFS Sensor: You can use the HdfsSensor to monitor HDFS for new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor\n",
    "\n",
    "hdfs_sensor = HdfsSensor(\n",
    "    task_id='wait_for_hdfs_file',\n",
    "    filepath='/user/hadoop/data_file.csv',\n",
    "    hdfs_conn_id='hdfs_default',\n",
    "    poke_interval=10,\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Integrating Airflow with Hadoop Cluster Components\n",
    "\n",
    "Now, letâ€™s integrate Airflow with components in your Hadoop cluster, such as HDFS, Hive, and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: HDFS, Hive, and Spark Integration\n",
    "\n",
    "1. HDFS Integration: Use `HdfsOperator` to move files between HDFS and local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.hdfs.operators.hdfs import HdfsPutFileOperator\n",
    "\n",
    "hdfs_put = HdfsPutFileOperator(\n",
    "    task_id='put_file_to_hdfs',\n",
    "    local_path='/tmp/processed_data.json',\n",
    "    remote_path='/user/hadoop/processed_data',\n",
    "    hdfs_conn_id='hdfs_default',\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hive Integration: Use `HiveOperator` to run Hive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.hive.operators.hive import HiveOperator\n",
    "\n",
    "hive_task = HiveOperator(\n",
    "    task_id='run_hive_query',\n",
    "    hql='SELECT * FROM sales_data LIMIT 10;',\n",
    "    hive_cli_conn_id='hive_conn',\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spark Integration: Use `SparkSubmitOperator` to run Spark jobs from Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "spark_task = SparkSubmitOperator(\n",
    "    task_id='run_spark_job',\n",
    "    application='/path/to/spark_job.py',\n",
    "    conn_id='spark_default',\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full Project: Implementing an ETL Pipeline\n",
    "\n",
    "**Overview of the Pipeline**\n",
    "- Extract: Use Airflow to extract data from an external API.\n",
    "- Transform: Use Spark to process the extracted data.\n",
    "- Load: Store the processed data in HDFS and load it into a Hive table.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before you begin, ensure that:\n",
    "\n",
    "- Airflow is up and running.\n",
    "- The Hadoop cluster (HDFS, Hive, and Spark) is set up and operational.\n",
    "- Spark is installed and configured on the cluster.\n",
    "- You have created a Hive database and a Hive table to store the processed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extract Data from an API\n",
    "\n",
    "In this step, you will use Airflowâ€™s `PythonOperator` to extract data from a public API and store it in a local file for further processing.\n",
    "\n",
    "    **Airflow DAG for Extraction**:\n",
    "- **Python function to extract data**: We will use the \"OpenWeather\" API to get weather data (you can choose any API). The data will be stored locally in JSON format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# fetches weather data from an external API and stores it as a JSON file in a specific directory.\n",
    "def extract_data():\n",
    "    url = \"https://api.open-meteo.com/v1/forecast?latitude=35&longitude=139&hourly=temperature_2m\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    with open('/tmp/weather_data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "# Define DAG default arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 9, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Instantiate the DAG\n",
    "dag = DAG(\n",
    "    'open_weather_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None #'@daily'\n",
    ")\n",
    "\n",
    "# Create the extraction task. PythonOperator calls this function as part of an Airflow DAG\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_weather_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transform the Data with Spark\n",
    "\n",
    "In this step, you will use a `SparkSubmitOperator` to transform the extracted data. Spark will read the JSON file and perform transformations (e.g., filtering, aggregating) to prepare it for loading into Hive.\n",
    "\n",
    "Spark Transformation Script (`transform_weather_data.py`):\n",
    "Save this Python script on your Hadoop cluster where Spark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeatherDataTransformation\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load raw JSON data\n",
    "df = spark.read.json(\"file:///tmp/weather_data.json\")\n",
    "\n",
    "# Perform transformations (e.g., filter rows, select columns)\n",
    "df_filtered = df.select(\n",
    "    explode(col(\"hourly.temperature_2m\")).alias(\"temperature\"),\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\")\n",
    ")\n",
    "\n",
    "# Save the transformed data to HDFS in Parquet format\n",
    "df_filtered.write.mode(\"append\").parquet(\"/user/datatech-labs/processed_weather_data\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airflow DAG for Transformation:**\n",
    "\n",
    "Now, add a new task in the Airflow DAG to submit this Spark job. And you also need to make sure to pip install Spark dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "# Define the Spark transformation task\n",
    "# transform_task = SparkSubmitOperator(\n",
    "#     task_id='transform_weather_data',\n",
    "#     application='/path/to/transform_weather_data.py',  # Replace with actual path to the script\n",
    "#     conn_id='spark_default',\n",
    "#     dag=dag\n",
    "# )\n",
    "\n",
    "# Or with normal bash command if you have an issue with installing SparkSubmitOperator:\n",
    "spark_file_path = '/home/oalsaghier/Documents/datatech_labs/datatech_lab_de_course_public/week5/pipelines_automation/airflow_dags_and_supported_files/pyspark_transform_weather_data.py'\n",
    "transform_task = BashOperator(\n",
    "    task_id='transform_weather_data',\n",
    "    bash_command=f'spark-submit --master yarn --deploy-mode client {spark_file_path}',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Set the task dependencies: extract first, then transform\n",
    "extract_task >> transform_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load Data into HDFS and Hive\n",
    "\n",
    "In this step, you will load the processed data from HDFS into a Hive table.\n",
    "\n",
    "**Hive Table Creation**:\n",
    "\n",
    "First, create a Hive table to store the data. Open the Hive CLI on your Hadoop cluster and run the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS weather_db;\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS weather_db.weather_data (\n",
    "    temperature DOUBLE,\n",
    "    latitude DOUBLE,\n",
    "    longitude DOUBLE\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/datatech-labs/processed_weather_data'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HOMEWORK: Airflow DAG for Loading Data into Hive:**\n",
    "\n",
    "Now, add a new task to your DAG to load data into Hive by running a Hive query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run this example, you need:\n",
    "#  1- install sasl on your linux machine:\n",
    "#   sudo apt-get install libsasl2-dev\n",
    "#  2- pip install the Hive operator package inside venv:\n",
    "#   pip install apache-airflow-providers-apache-hive\n",
    "#  3- go the connection, and edit 'hive_cli_default' connection by:\n",
    "#   setting '{\"use_beeline\": false}' in the 'Extra' field\n",
    "\n",
    "# from airflow.providers.apache.hive.operators.hive import HiveOperator\n",
    "\n",
    "# # Add Hive task to load data\n",
    "# load_task = HiveOperator(\n",
    "#     task_id='load_to_hive',\n",
    "#     hql='SELECT * FROM weather_db.weather_data;',  # Repair the Hive table to reflect new data\n",
    "#     hive_cli_conn_id='hive_conn',\n",
    "#     dag=dag\n",
    "# )\n",
    "\n",
    "hql_query = 'select * from weather_db.weather_data'\n",
    "load_task = BashOperator(\n",
    "    task_id='read_from_hive',\n",
    "    bash_command=f\"hive -e '{hql_query}'\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# Set task dependencies: transform first, then load\n",
    "transform_task >> load_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Monitoring the Pipeline\n",
    "\n",
    "**Airflow Monitoring**:\n",
    "\n",
    "You can monitor the progress of the DAG and its tasks through the Airflow UI. Check the logs for each task to see detailed information, including any errors that might occur.\n",
    "\n",
    "**Spark Monitoring**:\n",
    "\n",
    "To monitor the Spark job, use the Spark UI at `http://<spark-cluster-ip>:8088`. Here, you can view the stages and tasks of the Spark job, monitor execution time, and troubleshoot performance bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the ETL Pipeline:\n",
    "\n",
    "1. Extract: Fetch weather data from an API using Airflowâ€™s PythonOperator and store it in a local JSON file.\n",
    "\n",
    "2. Transform: Process the JSON data using Spark. The Spark job filters and processes the data, saving the result as a Parquet file in HDFS.\n",
    "\n",
    "3. Load: The processed data is loaded into a Hive table, making it available for querying and analysis.\n",
    "\n",
    "This full ETL pipeline is a practical demonstration of how Airflow can orchestrate complex data workflows, from data extraction to loading into a Hive data warehouse. By integrating Airflow with Spark and Hive, you achieve automated and scalable data processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
